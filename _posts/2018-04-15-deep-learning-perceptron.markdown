---
layout: post
title:  "ML/DL이 뭐에요? : 1탄. 퍼셉트론(perceptron)"
date:   2018-04-14 23:11:32 +0900
categories: mldl
---

퍼셉트론... 일단 이게 뭔지 알아야 공부를 시작할 수 있다고 해서 구글에 쳐 봤더니 위키피디아가 제일 먼저 뜬다.

> 퍼셉트론(perceptron)은 인공신경망의 한 종류로서, 1957년에 코넬 항공 연구소(Cornell Aeronautical Lab)의 프랑크 로젠블라트 (Frank Rosenblatt)에 의해 고안되었다. 이것은 가장 간단한 형태의 피드포워드(Feedforward) 네트워크 - 선형분류기 - 으로도 볼 수 있다.

무슨말인지 모르겠다... 좋은 연구소에서 멋진 사람들이 만든 좋은 이론인가보다. @_@

> 퍼셉트론이 동작하는 방식은 다음과 같다. 각 노드의 가중치와 입력치를 곱한 것을 모두 합한 값이 활성함수에 의해 판단되는데, 그 값이 임계치(보통 0)보다 크면 뉴런이 활성화되고 결과값으로 1을 출력한다. 뉴런이 활성화되지 않으면 결과값으로 -1을 출력한다.

음... 한 10%정도는 이해할 수 있을것 같다. 어떤 값이 어떤 기준을 넘어서면 1, 그렇지 않으면 0이라는건가? 그리고, "각 노드의 가중치"라는 말을 보자면 입력이 하나가 아닌것 같고 입력에 가중치가 곱해진 값을 임계치와 비교해서 활성화 여부를 결정하는것 같다.

> 마빈 민스키와 시모어 페퍼트는 저서 "퍼셉트론"에서 단층 퍼셉트론은 XOR 연산이 불가능하지만, 다층 퍼셉트론으로는 XOR 연산이 가능함을 보였다.

이걸 여러개 만들어서 겹치면 XOR 같은것도 구현할 수 있나보다.


### 입력값과 가중치(weight)

지금까지의 내용만 본다면, 퍼셉트론이란 n개의 입력을 받아 어떠한 규칙으로 활성화/비활성화 여부를 판단하는 함수를 사용하는 개념인 것 같다. 수식으로도 표현할 수 있는데 예를들어, 2개의 입력 $$x_1, x_2$$가 있고, 각 노드의 가중치를 $$w_1, w_2$$라고 하고, 임계치를 $$\theta$$라고 한다면 아래처럼 표현할 수 있다.

$$
y=
\begin{cases}
0, & (w_1x_1 + w_2x_2 \le \theta) \\
1, & (w_1x_1 + w_2x_2 \gt \theta)
\end{cases}
$$

이렇게 수식으로 표현해 놓고 보니 그리 어렵지만은 않아 보인다. 퍼셉트론을 이해하기 위해서는 가중치 뿐만 아니라 $$bias$$에 대해서도 알아야 한다.

### 편향(bios)

위 수식에서 임계치 $$\theta$$를 $$-b$$로 바꿔서 좌변으로 보내면 아래와 같이 바뀐다.

$$
y=
\begin{cases}
0, & (b + w_1x_1 + w_2x_2 \le 0) \\
1, & (b + w_1x_1 + w_2x_2 \gt 0)
\end{cases}
$$

변수 $$b$$가 바로 $$bias$$이다. $$bias$$는 편향이라는 뜻인데 단어의 뜻 그대로 "각 노드의 입력치에 가중치를 곱한 값을 모두 더한 값"에 추가적으로 더해지기 때문에 이 값에 따라 치우쳐지는 정도가 달라진다. n개의 입력값을 모두 동일한 기준으로 처리하지 않고 경우에 따라 차등을 주는 인자가 가중치라면, 편향은 퍼셉트론의 활성화 여부에 직접적으로 영향을 끼치는 인자이므로 이 변수의 값에 따라 결과가 크게 바뀔수도 있다.

### 활성화 함수(Activation function)

퍼셉트론의 활성화/비활성화를 결정하는 이 수식을 활성화 함수(Activation function)라고  하는데, 학창시절때 배운 기억을 더듬어 보면 이 퍼셉트론에 사용된 활성화 함수는 계단 함수이다.  입력값이 어느 임계치 미만이면 모두 0이고, 넘어서는 순간부터는 모두 1이되는 계단함수. 

![Image]({{ "/public/images/2018/20180415020942.png" | absolute_url }}){: width="50%" height="50%" .center-image}

위 그래프처럼 계단함수를 활성화 함수로 사용할 경우 어느 순간 갑자기 활성화가 되는데, 계단함수 대신 아래처럼 곡선으로 휘어진 함수(예: Sigmoid)를 사용하게 되면 계단함수 보다 매끄럽게 진행되므로 더 풍부한 결과를 얻을 수 있게 된다.

![Image]({{ "/public/images/2018/20180415180934.png" | absolute_url }}){: width="50%" height="50%" .center-image}

### 다층 퍼셉트론

한 개의 퍼셉트론으로는 "선형 분리 가능 문제"만 해결할 수 있다. 퍼셉트론에 대한 위키피디아의 설명중 가장 첫 문단에도 나와있듯이 퍼셉트론은 선형분류기이다. 공간을 하나의 선을 이용해서 2개의 영역으로 나눌 수 있기 때문에 AND나 OR같은 단순한 문제는 하나의 퍼셉트론이면 충분하다. 하지만 세상의 모든 객체들은 당연히 이렇게 깔끔하게 양분될 수 없다. 단적으로 XOR의 경우를 보아도 선 하나로는 구분할 수 없는데, 퍼셉트론을 하나 더 쌓아서 2층으로 구현하면 XOR도 구현할 수 있다. 이렇게 퍼셉트론을 여러개 겹쳐서 사용하는것을 다층 퍼셉트론이라고 하고 흔히 레이어를 쌓는다라고 표현한다.

다층 퍼셉트론을 사용할 때 주의해야 할 점은 활성화 함수로 반드시 비선형 함수를 사용해야 한다는 것이다. 선형 함수를 사용할 경우 레이어를 여러 겹으로 쌓아도 결국 하나의 레이어로 바꿔서 표현이 가능하기 때문에 레이어를 쌓아서 얻을 수 있는 잇점이 없다.

인공신경망을 이용한 딥러닝에서는 비선형 함수를 활성화 함수로 사용하는 퍼셉트론을 여러 겹으로 쌓아서 활용하는것이 포인트인데 레이어의 개수가 증가할수록 공간을 정교하게 나눌 수 있기 때문이다.


### 참고자료

* *Wikipedia, '퍼셉트론', https://ko.wikipedia.org/wiki/퍼셉트론*
* *사이토 고키, (2018), "Deep Learning from Scratch" (한빛미디어, 2018), p.47-75*
